FullyShardedDataParallel(
  (_fsdp_wrapped_module): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(50272, 2560, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)
        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-31): 32 x FullyShardedDataParallel(
            (_fsdp_wrapped_module): OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=2560, out_features=2560, bias=True)
                (v_proj): Linear(in_features=2560, out_features=2560, bias=True)
                (q_proj): Linear(in_features=2560, out_features=2560, bias=True)
                (out_proj): Linear(in_features=2560, out_features=2560, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=2560, out_features=10240, bias=True)
              (fc2): Linear(in_features=10240, out_features=2560, bias=True)
              (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
    )
    (lm_head): Linear(in_features=2560, out_features=50272, bias=False)
  )
)
